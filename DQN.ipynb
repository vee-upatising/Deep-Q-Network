{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Agent:\n",
    "    #\n",
    "    # Initializes attributes and constructs CNN model and target_model\n",
    "    #\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=5000)\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.gamma = 0.9            # Discount rate\n",
    "        self.epsilon = 1.0          # Exploration rate\n",
    "        self.epsilon_min = 0.1      # Minimal exploration rate (epsilon-greedy)\n",
    "        self.epsilon_decay = 0.995  # Decay rate for epsilon\n",
    "        self.update_rate = 1000     # Number of steps until updating the target network\n",
    "        \n",
    "        # Construct DQN models\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        self.model.summary()\n",
    "\n",
    "    #\n",
    "    # Constructs CNN\n",
    "    #\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Conv Layers\n",
    "        model.add(Conv2D(32, (8, 8), strides=4, padding='same', input_shape=self.state_size))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(64, (4, 4), strides=2, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides = (2,2)))\n",
    "        model.add(Conv2D(128, (3, 3), strides=1, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(128, (3, 3), strides=1, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides = (2,2)))\n",
    "        model.add(Flatten())\n",
    "\n",
    "        # FC Layers\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        \n",
    "        model.compile(loss='mse', optimizer=Adam())\n",
    "        return model\n",
    "\n",
    "    #\n",
    "    # Stores experience in replay memory\n",
    "    #\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    #\n",
    "    # Chooses action based on epsilon-greedy policy\n",
    "    #\n",
    "    def act(self, state):\n",
    "        # Random exploration\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        act_values = self.model.predict(state)\n",
    "        \n",
    "        return np.argmax(act_values[0])  # Returns action using policy\n",
    "\n",
    "    #\n",
    "    # Trains the model using randomly selected experiences in the replay memory\n",
    "    #\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            \n",
    "            if not done:\n",
    "                target = (reward + self.gamma * np.amax(self.target_model.predict(next_state)))\n",
    "            else:\n",
    "                target = reward\n",
    "                \n",
    "            # Construct the target vector as follows:\n",
    "            # 1. Use the current model to output the Q-value predictions\n",
    "            target_f = self.model.predict(state)\n",
    "            \n",
    "            # 2. Rewrite the chosen action value with the computed target\n",
    "            target_f[0][action] = target\n",
    "            \n",
    "            # 3. Use vectors in the objective computation\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    #\n",
    "    # Sets the target model parameters to the current model parameters\n",
    "    #\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "            \n",
    "    #\n",
    "    # Loads a saved model\n",
    "    #\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    #\n",
    "    # Saves parameters of a trained model\n",
    "    #\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful preprocessing taken from github.com/ageron/tiny-dqn\n",
    "def process_frame(frame):\n",
    "    mspacman_color = np.array([210, 164, 74]).mean()\n",
    "    img = frame[1:176:2, ::2]    # Crop and downsize\n",
    "    img = img.mean(axis=2)       # Convert to greyscale\n",
    "    img[img==mspacman_color] = 0 # Improve contrast by making pacman white\n",
    "    img = (img - 128) / 128 - 1  # Normalize from -1 to 1.\n",
    "    \n",
    "    return np.expand_dims(img.reshape(88, 80, 1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend_images(images, blend):\n",
    "    avg_image = np.expand_dims(np.zeros((88, 80, 1), np.float64), axis=0)\n",
    "\n",
    "    for image in images:\n",
    "        avg_image += image\n",
    "        \n",
    "    if len(images) < blend:\n",
    "        return avg_image / len(images)\n",
    "    else:\n",
    "        return avg_image / blend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 22, 20, 32)        2080      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 22, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 11, 10, 64)        32832     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 11, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 5, 5, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 5, 5, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 9)                 4617      \n",
      "=================================================================\n",
      "Total params: 786,281\n",
      "Trainable params: 786,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MsPacman-v0')\n",
    "state_size = (88, 80, 1)\n",
    "action_size = env.action_space.n\n",
    "agent = DQN_Agent(state_size, action_size)\n",
    "\n",
    "episodes = 800\n",
    "batch_size = 8\n",
    "skip_start = 90  # MsPacman-v0 waits for 90 actions before the episode begins\n",
    "total_time = 0   # Counter for total number of steps taken\n",
    "all_rewards = 0  # Used to compute avg reward over time\n",
    "blend = 4        # Number of images to blend\n",
    "done = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "episode: 1/800, game score: 240.0, reward: -206.0, avg reward: 240.0, time: 445, total time: 446\n",
      "episode: 2/800, game score: 310.0, reward: -343.0, avg reward: 275.0, time: 652, total time: 1099\n",
      "episode: 3/800, game score: 250.0, reward: -325.0, avg reward: 266.6666666666667, time: 574, total time: 1674\n",
      "episode: 4/800, game score: 760.0, reward: -291.0, avg reward: 390.0, time: 1050, total time: 2725\n",
      "episode: 5/800, game score: 380.0, reward: -161.0, avg reward: 388.0, time: 540, total time: 3266\n",
      "episode: 6/800, game score: 400.0, reward: -374.0, avg reward: 390.0, time: 773, total time: 4040\n",
      "episode: 7/800, game score: 690.0, reward: -383.0, avg reward: 432.85714285714283, time: 1072, total time: 5113\n",
      "episode: 8/800, game score: 650.0, reward: -42.0, avg reward: 460.0, time: 691, total time: 5805\n",
      "episode: 9/800, game score: 380.0, reward: -333.0, avg reward: 451.1111111111111, time: 712, total time: 6518\n",
      "episode: 10/800, game score: 450.0, reward: -421.0, avg reward: 451.0, time: 870, total time: 7389\n",
      "episode: 11/800, game score: 600.0, reward: 66.0, avg reward: 464.54545454545456, time: 533, total time: 7923\n",
      "episode: 12/800, game score: 800.0, reward: -57.0, avg reward: 492.5, time: 856, total time: 8780\n",
      "episode: 13/800, game score: 1050.0, reward: -46.0, avg reward: 535.3846153846154, time: 1095, total time: 9876\n",
      "episode: 14/800, game score: 380.0, reward: -234.0, avg reward: 524.2857142857143, time: 613, total time: 10490\n",
      "episode: 15/800, game score: 470.0, reward: -50.0, avg reward: 520.6666666666666, time: 519, total time: 11010\n",
      "episode: 16/800, game score: 510.0, reward: -184.0, avg reward: 520.0, time: 693, total time: 11704\n",
      "episode: 17/800, game score: 440.0, reward: -133.0, avg reward: 515.2941176470588, time: 572, total time: 12277\n",
      "episode: 18/800, game score: 730.0, reward: 49.0, avg reward: 527.2222222222222, time: 680, total time: 12958\n",
      "episode: 19/800, game score: 560.0, reward: -387.0, avg reward: 528.9473684210526, time: 946, total time: 13905\n",
      "episode: 20/800, game score: 770.0, reward: -190.0, avg reward: 541.0, time: 959, total time: 14865\n",
      "episode: 21/800, game score: 310.0, reward: -303.0, avg reward: 530.0, time: 612, total time: 15478\n",
      "episode: 22/800, game score: 640.0, reward: -108.0, avg reward: 535.0, time: 747, total time: 16226\n",
      "episode: 23/800, game score: 1560.0, reward: 480.0, avg reward: 579.5652173913044, time: 1079, total time: 17306\n",
      "episode: 24/800, game score: 1150.0, reward: 214.0, avg reward: 603.3333333333334, time: 935, total time: 18242\n",
      "episode: 25/800, game score: 440.0, reward: -47.0, avg reward: 596.8, time: 486, total time: 18729\n",
      "episode: 26/800, game score: 600.0, reward: -240.0, avg reward: 596.9230769230769, time: 839, total time: 19569\n",
      "episode: 27/800, game score: 460.0, reward: -74.0, avg reward: 591.8518518518518, time: 533, total time: 20103\n",
      "episode: 28/800, game score: 1080.0, reward: 384.0, avg reward: 609.2857142857143, time: 695, total time: 20799\n",
      "episode: 29/800, game score: 370.0, reward: -162.0, avg reward: 601.0344827586207, time: 531, total time: 21331\n",
      "episode: 30/800, game score: 1320.0, reward: 224.0, avg reward: 625.0, time: 1095, total time: 22427\n",
      "episode: 31/800, game score: 560.0, reward: -116.0, avg reward: 622.9032258064516, time: 675, total time: 23103\n",
      "episode: 32/800, game score: 660.0, reward: -134.0, avg reward: 624.0625, time: 793, total time: 23897\n",
      "episode: 33/800, game score: 260.0, reward: -302.0, avg reward: 613.030303030303, time: 561, total time: 24459\n",
      "episode: 34/800, game score: 590.0, reward: -292.0, avg reward: 612.3529411764706, time: 881, total time: 25341\n",
      "episode: 35/800, game score: 610.0, reward: -26.0, avg reward: 612.2857142857143, time: 635, total time: 25977\n",
      "episode: 36/800, game score: 830.0, reward: -28.0, avg reward: 618.3333333333334, time: 857, total time: 26835\n",
      "episode: 37/800, game score: 290.0, reward: -135.0, avg reward: 609.4594594594595, time: 424, total time: 27260\n",
      "episode: 38/800, game score: 920.0, reward: -31.0, avg reward: 617.6315789473684, time: 950, total time: 28211\n",
      "episode: 39/800, game score: 290.0, reward: -245.0, avg reward: 609.2307692307693, time: 534, total time: 28746\n",
      "episode: 40/800, game score: 430.0, reward: -216.0, avg reward: 604.75, time: 645, total time: 29392\n",
      "episode: 41/800, game score: 1890.0, reward: 886.0, avg reward: 636.0975609756098, time: 1003, total time: 30396\n",
      "episode: 42/800, game score: 1510.0, reward: 534.0, avg reward: 656.9047619047619, time: 975, total time: 31372\n",
      "episode: 43/800, game score: 460.0, reward: -31.0, avg reward: 652.3255813953489, time: 490, total time: 31863\n",
      "episode: 44/800, game score: 470.0, reward: -541.0, avg reward: 648.1818181818181, time: 1010, total time: 32874\n",
      "episode: 45/800, game score: 1180.0, reward: 313.0, avg reward: 660.0, time: 866, total time: 33741\n",
      "episode: 46/800, game score: 510.0, reward: -186.0, avg reward: 656.7391304347826, time: 695, total time: 34437\n",
      "episode: 47/800, game score: 430.0, reward: -184.0, avg reward: 651.9148936170212, time: 613, total time: 35051\n",
      "episode: 48/800, game score: 260.0, reward: -271.0, avg reward: 643.75, time: 530, total time: 35582\n",
      "episode: 49/800, game score: 1200.0, reward: 272.0, avg reward: 655.1020408163265, time: 927, total time: 36510\n",
      "episode: 50/800, game score: 470.0, reward: -21.0, avg reward: 651.4, time: 490, total time: 37001\n",
      "episode: 51/800, game score: 260.0, reward: -213.0, avg reward: 643.7254901960785, time: 472, total time: 37474\n",
      "episode: 52/800, game score: 360.0, reward: -134.0, avg reward: 638.2692307692307, time: 493, total time: 37968\n",
      "episode: 53/800, game score: 190.0, reward: -112.0, avg reward: 629.811320754717, time: 301, total time: 38270\n",
      "episode: 54/800, game score: 240.0, reward: -66.0, avg reward: 622.5925925925926, time: 305, total time: 38576\n",
      "episode: 55/800, game score: 370.0, reward: -147.0, avg reward: 618.0, time: 516, total time: 39093\n",
      "episode: 56/800, game score: 450.0, reward: -137.0, avg reward: 615.0, time: 586, total time: 39680\n",
      "episode: 57/800, game score: 350.0, reward: -299.0, avg reward: 610.3508771929825, time: 648, total time: 40329\n",
      "episode: 58/800, game score: 470.0, reward: -222.0, avg reward: 607.9310344827586, time: 691, total time: 41021\n",
      "episode: 59/800, game score: 640.0, reward: -80.0, avg reward: 608.4745762711865, time: 719, total time: 41741\n",
      "episode: 60/800, game score: 1300.0, reward: 320.0, avg reward: 620.0, time: 979, total time: 42721\n",
      "episode: 61/800, game score: 780.0, reward: 4.0, avg reward: 622.6229508196722, time: 775, total time: 43497\n",
      "episode: 62/800, game score: 500.0, reward: -174.0, avg reward: 620.6451612903226, time: 673, total time: 44171\n",
      "episode: 63/800, game score: 450.0, reward: -49.0, avg reward: 617.936507936508, time: 498, total time: 44670\n",
      "episode: 64/800, game score: 380.0, reward: -377.0, avg reward: 614.21875, time: 756, total time: 45427\n",
      "episode: 65/800, game score: 350.0, reward: -523.0, avg reward: 610.1538461538462, time: 872, total time: 46300\n",
      "episode: 66/800, game score: 270.0, reward: -77.0, avg reward: 605.0, time: 346, total time: 46647\n",
      "episode: 67/800, game score: 1090.0, reward: 22.0, avg reward: 612.2388059701492, time: 1067, total time: 47715\n",
      "episode: 68/800, game score: 900.0, reward: -129.0, avg reward: 616.4705882352941, time: 1028, total time: 48744\n",
      "episode: 69/800, game score: 590.0, reward: -72.0, avg reward: 616.0869565217391, time: 661, total time: 49406\n",
      "episode: 70/800, game score: 1800.0, reward: 744.0, avg reward: 633.0, time: 1055, total time: 50462\n",
      "episode: 71/800, game score: 1030.0, reward: 286.0, avg reward: 638.5915492957746, time: 743, total time: 51206\n",
      "episode: 72/800, game score: 1050.0, reward: 362.0, avg reward: 644.3055555555555, time: 687, total time: 51894\n",
      "episode: 73/800, game score: 400.0, reward: -428.0, avg reward: 640.9589041095891, time: 827, total time: 52722\n",
      "episode: 74/800, game score: 360.0, reward: -200.0, avg reward: 637.1621621621622, time: 559, total time: 53282\n",
      "episode: 75/800, game score: 430.0, reward: -213.0, avg reward: 634.4, time: 642, total time: 53925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 76/800, game score: 620.0, reward: -5.0, avg reward: 634.2105263157895, time: 624, total time: 54550\n",
      "episode: 77/800, game score: 200.0, reward: -110.0, avg reward: 628.5714285714286, time: 309, total time: 54860\n",
      "episode: 78/800, game score: 310.0, reward: -45.0, avg reward: 624.4871794871794, time: 354, total time: 55215\n",
      "episode: 79/800, game score: 270.0, reward: -63.0, avg reward: 620.0, time: 332, total time: 55548\n",
      "episode: 80/800, game score: 880.0, reward: 64.0, avg reward: 623.25, time: 815, total time: 56364\n",
      "episode: 81/800, game score: 360.0, reward: -186.0, avg reward: 620.0, time: 545, total time: 56910\n",
      "episode: 82/800, game score: 740.0, reward: -104.0, avg reward: 621.4634146341464, time: 843, total time: 57754\n",
      "episode: 83/800, game score: 380.0, reward: -123.0, avg reward: 618.5542168674699, time: 502, total time: 58257\n",
      "episode: 84/800, game score: 230.0, reward: -169.0, avg reward: 613.9285714285714, time: 398, total time: 58656\n",
      "episode: 85/800, game score: 720.0, reward: 50.0, avg reward: 615.1764705882352, time: 669, total time: 59326\n",
      "episode: 86/800, game score: 1190.0, reward: 63.0, avg reward: 621.8604651162791, time: 1126, total time: 60453\n",
      "episode: 87/800, game score: 340.0, reward: -184.0, avg reward: 618.6206896551724, time: 523, total time: 60977\n",
      "episode: 88/800, game score: 420.0, reward: -175.0, avg reward: 616.3636363636364, time: 594, total time: 61572\n",
      "episode: 89/800, game score: 1320.0, reward: 369.0, avg reward: 624.2696629213483, time: 950, total time: 62523\n",
      "episode: 90/800, game score: 430.0, reward: -66.0, avg reward: 622.1111111111111, time: 495, total time: 63019\n",
      "episode: 91/800, game score: 540.0, reward: -157.0, avg reward: 621.2087912087912, time: 696, total time: 63716\n",
      "episode: 92/800, game score: 650.0, reward: -50.0, avg reward: 621.5217391304348, time: 699, total time: 64416\n",
      "episode: 93/800, game score: 250.0, reward: -108.0, avg reward: 617.5268817204301, time: 357, total time: 64774\n",
      "episode: 94/800, game score: 390.0, reward: -176.0, avg reward: 615.1063829787234, time: 565, total time: 65340\n",
      "episode: 95/800, game score: 1200.0, reward: -45.0, avg reward: 621.2631578947369, time: 1244, total time: 66585\n",
      "episode: 96/800, game score: 550.0, reward: -331.0, avg reward: 620.5208333333334, time: 880, total time: 67466\n",
      "episode: 97/800, game score: 820.0, reward: -209.0, avg reward: 622.5773195876288, time: 1028, total time: 68495\n",
      "episode: 98/800, game score: 410.0, reward: -479.0, avg reward: 620.4081632653061, time: 888, total time: 69384\n",
      "episode: 99/800, game score: 440.0, reward: -158.0, avg reward: 618.5858585858585, time: 597, total time: 69982\n",
      "episode: 100/800, game score: 760.0, reward: 133.0, avg reward: 620.0, time: 626, total time: 70609\n",
      "episode: 101/800, game score: 630.0, reward: -88.0, avg reward: 620.0990099009902, time: 717, total time: 71327\n",
      "episode: 102/800, game score: 450.0, reward: -66.0, avg reward: 618.4313725490196, time: 515, total time: 71843\n",
      "episode: 103/800, game score: 260.0, reward: -122.0, avg reward: 614.9514563106796, time: 381, total time: 72225\n",
      "episode: 104/800, game score: 790.0, reward: -180.0, avg reward: 616.6346153846154, time: 969, total time: 73195\n",
      "episode: 105/800, game score: 300.0, reward: -264.0, avg reward: 613.6190476190476, time: 563, total time: 73759\n",
      "episode: 106/800, game score: 810.0, reward: 99.0, avg reward: 615.4716981132076, time: 710, total time: 74470\n",
      "episode: 107/800, game score: 840.0, reward: -138.0, avg reward: 617.5700934579439, time: 977, total time: 75448\n",
      "episode: 108/800, game score: 370.0, reward: -265.0, avg reward: 615.2777777777778, time: 634, total time: 76083\n",
      "episode: 109/800, game score: 390.0, reward: -258.0, avg reward: 613.2110091743119, time: 647, total time: 76731\n",
      "episode: 110/800, game score: 230.0, reward: -185.0, avg reward: 609.7272727272727, time: 414, total time: 77146\n",
      "episode: 111/800, game score: 1340.0, reward: 642.0, avg reward: 616.3063063063063, time: 697, total time: 77844\n",
      "episode: 112/800, game score: 610.0, reward: -266.0, avg reward: 616.25, time: 875, total time: 78720\n",
      "episode: 113/800, game score: 370.0, reward: -195.0, avg reward: 614.070796460177, time: 564, total time: 79285\n",
      "episode: 114/800, game score: 410.0, reward: -125.0, avg reward: 612.280701754386, time: 534, total time: 79820\n",
      "episode: 115/800, game score: 1070.0, reward: 160.0, avg reward: 616.2608695652174, time: 909, total time: 80730\n",
      "episode: 116/800, game score: 420.0, reward: -79.0, avg reward: 614.5689655172414, time: 498, total time: 81229\n",
      "episode: 117/800, game score: 230.0, reward: -354.0, avg reward: 611.2820512820513, time: 583, total time: 81813\n",
      "episode: 118/800, game score: 400.0, reward: -326.0, avg reward: 609.4915254237288, time: 725, total time: 82539\n",
      "episode: 119/800, game score: 360.0, reward: -646.0, avg reward: 607.3949579831933, time: 1005, total time: 83545\n",
      "episode: 120/800, game score: 1100.0, reward: 267.0, avg reward: 611.5, time: 832, total time: 84378\n",
      "episode: 121/800, game score: 250.0, reward: -469.0, avg reward: 608.5123966942149, time: 718, total time: 85097\n",
      "episode: 122/800, game score: 390.0, reward: -323.0, avg reward: 606.7213114754098, time: 712, total time: 85810\n",
      "episode: 123/800, game score: 1760.0, reward: 823.0, avg reward: 616.0975609756098, time: 936, total time: 86747\n",
      "episode: 124/800, game score: 500.0, reward: -404.0, avg reward: 615.1612903225806, time: 903, total time: 87651\n",
      "episode: 125/800, game score: 300.0, reward: -235.0, avg reward: 612.64, time: 534, total time: 88186\n",
      "episode: 126/800, game score: 260.0, reward: -193.0, avg reward: 609.8412698412699, time: 452, total time: 88639\n",
      "episode: 127/800, game score: 660.0, reward: -227.0, avg reward: 610.2362204724409, time: 886, total time: 89526\n",
      "episode: 128/800, game score: 400.0, reward: -264.0, avg reward: 608.59375, time: 663, total time: 90190\n",
      "episode: 129/800, game score: 370.0, reward: -307.0, avg reward: 606.7441860465116, time: 676, total time: 90867\n",
      "episode: 130/800, game score: 250.0, reward: -184.0, avg reward: 604.0, time: 433, total time: 91301\n",
      "episode: 131/800, game score: 410.0, reward: -172.0, avg reward: 602.5190839694657, time: 581, total time: 91883\n",
      "episode: 132/800, game score: 180.0, reward: -407.0, avg reward: 599.3181818181819, time: 586, total time: 92470\n",
      "episode: 133/800, game score: 420.0, reward: -577.0, avg reward: 597.9699248120301, time: 996, total time: 93467\n",
      "episode: 134/800, game score: 330.0, reward: -401.0, avg reward: 595.9701492537314, time: 730, total time: 94198\n",
      "episode: 135/800, game score: 490.0, reward: -52.0, avg reward: 595.1851851851852, time: 541, total time: 94740\n",
      "episode: 136/800, game score: 2060.0, reward: 1068.0, avg reward: 605.9558823529412, time: 991, total time: 95732\n",
      "episode: 137/800, game score: 490.0, reward: -65.0, avg reward: 605.1094890510949, time: 554, total time: 96287\n",
      "episode: 138/800, game score: 1040.0, reward: 115.0, avg reward: 608.2608695652174, time: 924, total time: 97212\n",
      "episode: 139/800, game score: 550.0, reward: -143.0, avg reward: 607.841726618705, time: 692, total time: 97905\n",
      "episode: 140/800, game score: 790.0, reward: 98.0, avg reward: 609.1428571428571, time: 691, total time: 98597\n",
      "episode: 141/800, game score: 820.0, reward: -36.0, avg reward: 610.6382978723404, time: 855, total time: 99453\n",
      "episode: 142/800, game score: 960.0, reward: 194.0, avg reward: 613.0985915492957, time: 765, total time: 100219\n",
      "episode: 143/800, game score: 810.0, reward: -432.0, avg reward: 614.4755244755245, time: 1241, total time: 101461\n",
      "episode: 144/800, game score: 200.0, reward: -485.0, avg reward: 611.5972222222222, time: 684, total time: 102146\n",
      "episode: 145/800, game score: 310.0, reward: -130.0, avg reward: 609.5172413793103, time: 439, total time: 102586\n",
      "episode: 146/800, game score: 450.0, reward: -109.0, avg reward: 608.4246575342465, time: 558, total time: 103145\n",
      "episode: 147/800, game score: 320.0, reward: -195.0, avg reward: 606.4625850340136, time: 514, total time: 103660\n",
      "episode: 148/800, game score: 440.0, reward: -155.0, avg reward: 605.3378378378378, time: 594, total time: 104255\n",
      "episode: 149/800, game score: 410.0, reward: -270.0, avg reward: 604.0268456375838, time: 679, total time: 104935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 150/800, game score: 380.0, reward: -219.0, avg reward: 602.5333333333333, time: 598, total time: 105534\n",
      "episode: 151/800, game score: 420.0, reward: -440.0, avg reward: 601.3245033112582, time: 859, total time: 106394\n",
      "episode: 152/800, game score: 310.0, reward: -283.0, avg reward: 599.4078947368421, time: 592, total time: 106987\n",
      "episode: 153/800, game score: 140.0, reward: -657.0, avg reward: 596.40522875817, time: 796, total time: 107784\n",
      "episode: 154/800, game score: 670.0, reward: 96.0, avg reward: 596.8831168831168, time: 573, total time: 108358\n",
      "episode: 155/800, game score: 460.0, reward: -206.0, avg reward: 596.0, time: 665, total time: 109024\n",
      "episode: 156/800, game score: 510.0, reward: -132.0, avg reward: 595.4487179487179, time: 641, total time: 109666\n",
      "episode: 157/800, game score: 410.0, reward: -406.0, avg reward: 594.2675159235669, time: 815, total time: 110482\n",
      "episode: 158/800, game score: 790.0, reward: 20.0, avg reward: 595.506329113924, time: 769, total time: 111252\n",
      "episode: 159/800, game score: 210.0, reward: -390.0, avg reward: 593.0817610062893, time: 599, total time: 111852\n",
      "episode: 160/800, game score: 430.0, reward: -190.0, avg reward: 592.0625, time: 619, total time: 112472\n",
      "episode: 161/800, game score: 610.0, reward: -82.0, avg reward: 592.1739130434783, time: 691, total time: 113164\n",
      "episode: 162/800, game score: 350.0, reward: -167.0, avg reward: 590.679012345679, time: 516, total time: 113681\n",
      "episode: 163/800, game score: 400.0, reward: -299.0, avg reward: 589.5092024539878, time: 698, total time: 114380\n",
      "episode: 164/800, game score: 320.0, reward: -230.0, avg reward: 587.8658536585366, time: 549, total time: 114930\n",
      "episode: 165/800, game score: 430.0, reward: -145.0, avg reward: 586.9090909090909, time: 574, total time: 115505\n",
      "episode: 166/800, game score: 1300.0, reward: 340.0, avg reward: 591.2048192771084, time: 959, total time: 116465\n",
      "episode: 167/800, game score: 450.0, reward: -99.0, avg reward: 590.3592814371258, time: 548, total time: 117014\n",
      "episode: 168/800, game score: 270.0, reward: -279.0, avg reward: 588.452380952381, time: 548, total time: 117563\n",
      "episode: 169/800, game score: 640.0, reward: -254.0, avg reward: 588.7573964497042, time: 893, total time: 118457\n",
      "episode: 170/800, game score: 280.0, reward: -262.0, avg reward: 586.9411764705883, time: 541, total time: 118999\n",
      "episode: 171/800, game score: 1910.0, reward: 1145.0, avg reward: 594.6783625730994, time: 764, total time: 119764\n",
      "episode: 172/800, game score: 370.0, reward: -214.0, avg reward: 593.3720930232558, time: 583, total time: 120348\n",
      "episode: 173/800, game score: 530.0, reward: -380.0, avg reward: 593.0057803468208, time: 909, total time: 121258\n",
      "episode: 174/800, game score: 530.0, reward: -166.0, avg reward: 592.6436781609195, time: 695, total time: 121954\n",
      "episode: 175/800, game score: 290.0, reward: -267.0, avg reward: 590.9142857142857, time: 556, total time: 122511\n",
      "episode: 176/800, game score: 270.0, reward: -353.0, avg reward: 589.0909090909091, time: 622, total time: 123134\n",
      "episode: 177/800, game score: 360.0, reward: -265.0, avg reward: 587.7966101694915, time: 624, total time: 123759\n",
      "episode: 178/800, game score: 320.0, reward: -305.0, avg reward: 586.2921348314607, time: 624, total time: 124384\n",
      "episode: 179/800, game score: 340.0, reward: -305.0, avg reward: 584.9162011173185, time: 644, total time: 125029\n",
      "episode: 180/800, game score: 240.0, reward: -82.0, avg reward: 583.0, time: 321, total time: 125351\n",
      "episode: 181/800, game score: 330.0, reward: -268.0, avg reward: 581.6022099447514, time: 597, total time: 125949\n",
      "episode: 182/800, game score: 210.0, reward: -365.0, avg reward: 579.5604395604396, time: 574, total time: 126524\n",
      "episode: 183/800, game score: 270.0, reward: -157.0, avg reward: 577.8688524590164, time: 426, total time: 126951\n",
      "episode: 184/800, game score: 340.0, reward: -252.0, avg reward: 576.5760869565217, time: 591, total time: 127543\n",
      "episode: 185/800, game score: 240.0, reward: -263.0, avg reward: 574.7567567567568, time: 502, total time: 128046\n",
      "episode: 186/800, game score: 960.0, reward: 214.0, avg reward: 576.8279569892474, time: 745, total time: 128792\n",
      "episode: 187/800, game score: 400.0, reward: -282.0, avg reward: 575.8823529411765, time: 681, total time: 129474\n",
      "episode: 188/800, game score: 430.0, reward: -141.0, avg reward: 575.1063829787234, time: 570, total time: 130045\n",
      "episode: 189/800, game score: 1000.0, reward: 268.0, avg reward: 577.3544973544973, time: 731, total time: 130777\n",
      "episode: 190/800, game score: 550.0, reward: 8.0, avg reward: 577.2105263157895, time: 541, total time: 131319\n",
      "episode: 191/800, game score: 210.0, reward: -395.0, avg reward: 575.2879581151833, time: 604, total time: 131924\n",
      "episode: 192/800, game score: 560.0, reward: -229.0, avg reward: 575.2083333333334, time: 788, total time: 132713\n",
      "episode: 193/800, game score: 480.0, reward: -254.0, avg reward: 574.7150259067357, time: 733, total time: 133447\n",
      "episode: 194/800, game score: 660.0, reward: -309.0, avg reward: 575.1546391752578, time: 968, total time: 134416\n",
      "episode: 195/800, game score: 410.0, reward: -379.0, avg reward: 574.3076923076923, time: 788, total time: 135205\n",
      "episode: 196/800, game score: 660.0, reward: -185.0, avg reward: 574.7448979591836, time: 844, total time: 136050\n",
      "episode: 197/800, game score: 330.0, reward: -250.0, avg reward: 573.5025380710659, time: 579, total time: 136630\n",
      "episode: 198/800, game score: 200.0, reward: -175.0, avg reward: 571.6161616161617, time: 374, total time: 137005\n",
      "episode: 199/800, game score: 480.0, reward: -104.0, avg reward: 571.1557788944724, time: 583, total time: 137589\n",
      "episode: 200/800, game score: 580.0, reward: -133.0, avg reward: 571.2, time: 712, total time: 138302\n",
      "episode: 201/800, game score: 390.0, reward: -260.0, avg reward: 570.2985074626865, time: 649, total time: 138952\n",
      "episode: 202/800, game score: 220.0, reward: -198.0, avg reward: 568.5643564356436, time: 417, total time: 139370\n",
      "episode: 203/800, game score: 320.0, reward: -182.0, avg reward: 567.3399014778325, time: 501, total time: 139872\n",
      "episode: 204/800, game score: 420.0, reward: -183.0, avg reward: 566.6176470588235, time: 602, total time: 140475\n",
      "episode: 205/800, game score: 390.0, reward: -338.0, avg reward: 565.7560975609756, time: 727, total time: 141203\n",
      "episode: 206/800, game score: 400.0, reward: -648.0, avg reward: 564.9514563106796, time: 1047, total time: 142251\n",
      "episode: 207/800, game score: 910.0, reward: -296.0, avg reward: 566.6183574879227, time: 1205, total time: 143457\n",
      "episode: 208/800, game score: 430.0, reward: -50.0, avg reward: 565.9615384615385, time: 479, total time: 143937\n",
      "episode: 209/800, game score: 480.0, reward: -433.0, avg reward: 565.5502392344498, time: 912, total time: 144850\n",
      "episode: 210/800, game score: 590.0, reward: -221.0, avg reward: 565.6666666666666, time: 810, total time: 145661\n",
      "episode: 211/800, game score: 280.0, reward: -234.0, avg reward: 564.3127962085308, time: 513, total time: 146175\n",
      "episode: 212/800, game score: 370.0, reward: -163.0, avg reward: 563.3962264150944, time: 532, total time: 146708\n",
      "episode: 213/800, game score: 340.0, reward: -449.0, avg reward: 562.3474178403756, time: 788, total time: 147497\n",
      "episode: 214/800, game score: 960.0, reward: 169.0, avg reward: 564.2056074766355, time: 790, total time: 148288\n",
      "episode: 215/800, game score: 620.0, reward: -134.0, avg reward: 564.4651162790698, time: 753, total time: 149042\n",
      "episode: 216/800, game score: 390.0, reward: -247.0, avg reward: 563.6574074074074, time: 636, total time: 149679\n",
      "episode: 217/800, game score: 340.0, reward: -253.0, avg reward: 562.6267281105991, time: 592, total time: 150272\n",
      "episode: 218/800, game score: 320.0, reward: -154.0, avg reward: 561.5137614678899, time: 473, total time: 150746\n",
      "episode: 219/800, game score: 290.0, reward: -316.0, avg reward: 560.2739726027397, time: 605, total time: 151352\n",
      "episode: 220/800, game score: 250.0, reward: -178.0, avg reward: 558.8636363636364, time: 427, total time: 151780\n",
      "episode: 221/800, game score: 670.0, reward: -92.0, avg reward: 559.3665158371041, time: 761, total time: 152542\n",
      "episode: 222/800, game score: 1360.0, reward: 529.0, avg reward: 562.972972972973, time: 830, total time: 153373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 223/800, game score: 550.0, reward: -195.0, avg reward: 562.9147982062781, time: 744, total time: 154118\n",
      "episode: 224/800, game score: 780.0, reward: -43.0, avg reward: 563.8839285714286, time: 822, total time: 154941\n",
      "episode: 225/800, game score: 210.0, reward: -101.0, avg reward: 562.3111111111111, time: 310, total time: 155252\n",
      "episode: 226/800, game score: 370.0, reward: -305.0, avg reward: 561.4601769911504, time: 674, total time: 155927\n",
      "episode: 227/800, game score: 1150.0, reward: 414.0, avg reward: 564.0528634361234, time: 735, total time: 156663\n",
      "episode: 228/800, game score: 290.0, reward: -194.0, avg reward: 562.8508771929825, time: 483, total time: 157147\n",
      "episode: 229/800, game score: 260.0, reward: -386.0, avg reward: 561.528384279476, time: 645, total time: 157793\n",
      "episode: 230/800, game score: 540.0, reward: -173.0, avg reward: 561.4347826086956, time: 712, total time: 158506\n",
      "episode: 231/800, game score: 480.0, reward: -191.0, avg reward: 561.0822510822511, time: 670, total time: 159177\n",
      "episode: 232/800, game score: 320.0, reward: -179.0, avg reward: 560.0431034482758, time: 498, total time: 159676\n",
      "episode: 233/800, game score: 740.0, reward: -183.0, avg reward: 560.8154506437768, time: 922, total time: 160599\n",
      "episode: 234/800, game score: 570.0, reward: 36.0, avg reward: 560.8547008547008, time: 533, total time: 161133\n",
      "episode: 235/800, game score: 240.0, reward: -178.0, avg reward: 559.4893617021277, time: 417, total time: 161551\n",
      "episode: 236/800, game score: 360.0, reward: -171.0, avg reward: 558.6440677966102, time: 530, total time: 162082\n",
      "episode: 237/800, game score: 400.0, reward: -148.0, avg reward: 557.9746835443038, time: 547, total time: 162630\n",
      "episode: 238/800, game score: 510.0, reward: -368.0, avg reward: 557.7731092436975, time: 877, total time: 163508\n",
      "episode: 239/800, game score: 400.0, reward: -284.0, avg reward: 557.1129707112971, time: 683, total time: 164192\n",
      "episode: 240/800, game score: 580.0, reward: -26.0, avg reward: 557.2083333333334, time: 605, total time: 164798\n",
      "episode: 241/800, game score: 240.0, reward: -85.0, avg reward: 555.8921161825726, time: 324, total time: 165123\n",
      "episode: 242/800, game score: 760.0, reward: 19.0, avg reward: 556.7355371900826, time: 740, total time: 165864\n",
      "episode: 243/800, game score: 310.0, reward: -339.0, avg reward: 555.7201646090535, time: 648, total time: 166513\n",
      "episode: 244/800, game score: 530.0, reward: -138.0, avg reward: 555.6147540983607, time: 667, total time: 167181\n",
      "episode: 245/800, game score: 360.0, reward: -89.0, avg reward: 554.8163265306123, time: 448, total time: 167630\n",
      "episode: 246/800, game score: 980.0, reward: 238.0, avg reward: 556.5447154471544, time: 741, total time: 168372\n",
      "episode: 247/800, game score: 300.0, reward: -217.0, avg reward: 555.506072874494, time: 516, total time: 168889\n",
      "episode: 248/800, game score: 430.0, reward: -181.0, avg reward: 555.0, time: 610, total time: 169500\n",
      "episode: 249/800, game score: 580.0, reward: -101.0, avg reward: 555.1004016064257, time: 680, total time: 170181\n",
      "episode: 250/800, game score: 410.0, reward: -126.0, avg reward: 554.52, time: 535, total time: 170717\n",
      "episode: 251/800, game score: 320.0, reward: -482.0, avg reward: 553.5856573705179, time: 801, total time: 171519\n",
      "episode: 252/800, game score: 740.0, reward: -274.0, avg reward: 554.3253968253969, time: 1013, total time: 172533\n",
      "episode: 253/800, game score: 380.0, reward: -117.0, avg reward: 553.6363636363636, time: 496, total time: 173030\n",
      "episode: 254/800, game score: 580.0, reward: -331.0, avg reward: 553.7401574803149, time: 910, total time: 173941\n",
      "episode: 255/800, game score: 590.0, reward: -112.0, avg reward: 553.8823529411765, time: 701, total time: 174643\n",
      "episode: 256/800, game score: 440.0, reward: -129.0, avg reward: 553.4375, time: 568, total time: 175212\n",
      "episode: 257/800, game score: 430.0, reward: -64.0, avg reward: 552.9571984435797, time: 493, total time: 175706\n",
      "episode: 258/800, game score: 1140.0, reward: 371.0, avg reward: 555.2325581395348, time: 768, total time: 176475\n",
      "episode: 259/800, game score: 760.0, reward: -112.0, avg reward: 556.023166023166, time: 871, total time: 177347\n",
      "episode: 260/800, game score: 390.0, reward: -366.0, avg reward: 555.3846153846154, time: 755, total time: 178103\n",
      "episode: 261/800, game score: 440.0, reward: -71.0, avg reward: 554.9425287356322, time: 510, total time: 178614\n",
      "episode: 262/800, game score: 540.0, reward: -110.0, avg reward: 554.8854961832061, time: 649, total time: 179264\n",
      "episode: 263/800, game score: 490.0, reward: -225.0, avg reward: 554.638783269962, time: 714, total time: 179979\n",
      "episode: 264/800, game score: 190.0, reward: -164.0, avg reward: 553.2575757575758, time: 353, total time: 180333\n",
      "episode: 265/800, game score: 210.0, reward: -137.0, avg reward: 551.9622641509434, time: 346, total time: 180680\n",
      "episode: 266/800, game score: 210.0, reward: -340.0, avg reward: 550.6766917293234, time: 549, total time: 181230\n",
      "episode: 267/800, game score: 610.0, reward: -128.0, avg reward: 550.8988764044943, time: 737, total time: 181968\n",
      "episode: 268/800, game score: 950.0, reward: -113.0, avg reward: 552.3880597014926, time: 1062, total time: 183031\n",
      "episode: 269/800, game score: 300.0, reward: -88.0, avg reward: 551.449814126394, time: 387, total time: 183419\n",
      "episode: 270/800, game score: 520.0, reward: -144.0, avg reward: 551.3333333333334, time: 663, total time: 184083\n",
      "episode: 271/800, game score: 510.0, reward: -276.0, avg reward: 551.180811808118, time: 785, total time: 184869\n",
      "episode: 272/800, game score: 510.0, reward: -327.0, avg reward: 551.0294117647059, time: 836, total time: 185706\n",
      "episode: 273/800, game score: 250.0, reward: -116.0, avg reward: 549.92673992674, time: 365, total time: 186072\n",
      "episode: 274/800, game score: 450.0, reward: -387.0, avg reward: 549.5620437956204, time: 836, total time: 186909\n",
      "episode: 275/800, game score: 220.0, reward: -561.0, avg reward: 548.3636363636364, time: 780, total time: 187690\n",
      "episode: 276/800, game score: 500.0, reward: -16.0, avg reward: 548.1884057971015, time: 515, total time: 188206\n",
      "episode: 277/800, game score: 210.0, reward: -287.0, avg reward: 546.9675090252707, time: 496, total time: 188703\n",
      "episode: 278/800, game score: 380.0, reward: -122.0, avg reward: 546.3669064748201, time: 501, total time: 189205\n",
      "episode: 279/800, game score: 660.0, reward: -74.0, avg reward: 546.7741935483871, time: 733, total time: 189939\n",
      "episode: 280/800, game score: 530.0, reward: -243.0, avg reward: 546.7142857142857, time: 772, total time: 190712\n",
      "episode: 281/800, game score: 740.0, reward: -81.0, avg reward: 547.4021352313167, time: 820, total time: 191533\n",
      "episode: 282/800, game score: 200.0, reward: -408.0, avg reward: 546.1702127659574, time: 607, total time: 192141\n",
      "episode: 283/800, game score: 670.0, reward: -138.0, avg reward: 546.6077738515901, time: 807, total time: 192949\n",
      "episode: 284/800, game score: 570.0, reward: -29.0, avg reward: 546.6901408450705, time: 598, total time: 193548\n",
      "episode: 285/800, game score: 510.0, reward: -49.0, avg reward: 546.561403508772, time: 558, total time: 194107\n",
      "episode: 286/800, game score: 1110.0, reward: 259.0, avg reward: 548.5314685314685, time: 850, total time: 194958\n",
      "episode: 287/800, game score: 420.0, reward: -92.0, avg reward: 548.0836236933798, time: 511, total time: 195470\n",
      "episode: 288/800, game score: 560.0, reward: -159.0, avg reward: 548.125, time: 718, total time: 196189\n",
      "episode: 289/800, game score: 700.0, reward: -126.0, avg reward: 548.6505190311418, time: 825, total time: 197015\n",
      "episode: 290/800, game score: 410.0, reward: -273.0, avg reward: 548.1724137931035, time: 682, total time: 197698\n",
      "episode: 291/800, game score: 780.0, reward: -287.0, avg reward: 548.9690721649484, time: 1066, total time: 198765\n",
      "episode: 292/800, game score: 390.0, reward: -97.0, avg reward: 548.4246575342465, time: 486, total time: 199252\n",
      "episode: 293/800, game score: 630.0, reward: -219.0, avg reward: 548.7030716723549, time: 848, total time: 200101\n",
      "episode: 294/800, game score: 460.0, reward: -113.0, avg reward: 548.4013605442177, time: 572, total time: 200674\n",
      "episode: 295/800, game score: 500.0, reward: -228.0, avg reward: 548.2372881355932, time: 727, total time: 201402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 296/800, game score: 280.0, reward: -179.0, avg reward: 547.331081081081, time: 458, total time: 201861\n",
      "episode: 297/800, game score: 1230.0, reward: 352.0, avg reward: 549.6296296296297, time: 877, total time: 202739\n",
      "episode: 298/800, game score: 260.0, reward: -180.0, avg reward: 548.6577181208054, time: 439, total time: 203179\n",
      "episode: 299/800, game score: 440.0, reward: -195.0, avg reward: 548.2943143812709, time: 634, total time: 203814\n",
      "episode: 300/800, game score: 1040.0, reward: 297.0, avg reward: 549.9333333333333, time: 742, total time: 204557\n",
      "episode: 301/800, game score: 470.0, reward: -104.0, avg reward: 549.6677740863787, time: 573, total time: 205131\n",
      "episode: 302/800, game score: 320.0, reward: -301.0, avg reward: 548.9072847682119, time: 620, total time: 205752\n",
      "episode: 303/800, game score: 350.0, reward: -480.0, avg reward: 548.2508250825083, time: 829, total time: 206582\n",
      "episode: 304/800, game score: 1090.0, reward: 439.0, avg reward: 550.0328947368421, time: 650, total time: 207233\n",
      "episode: 305/800, game score: 410.0, reward: -306.0, avg reward: 549.5737704918033, time: 715, total time: 207949\n",
      "episode: 306/800, game score: 310.0, reward: -147.0, avg reward: 548.7908496732026, time: 456, total time: 208406\n",
      "episode: 307/800, game score: 300.0, reward: -173.0, avg reward: 547.9804560260586, time: 472, total time: 208879\n",
      "episode: 308/800, game score: 690.0, reward: -387.0, avg reward: 548.4415584415584, time: 1076, total time: 209956\n",
      "episode: 309/800, game score: 930.0, reward: 302.0, avg reward: 549.6763754045307, time: 627, total time: 210584\n",
      "episode: 310/800, game score: 320.0, reward: -471.0, avg reward: 548.9354838709677, time: 790, total time: 211375\n",
      "episode: 311/800, game score: 240.0, reward: -148.0, avg reward: 547.9421221864952, time: 387, total time: 211763\n",
      "episode: 312/800, game score: 310.0, reward: -75.0, avg reward: 547.1794871794872, time: 384, total time: 212148\n",
      "episode: 313/800, game score: 490.0, reward: -121.0, avg reward: 546.9968051118211, time: 610, total time: 212759\n",
      "episode: 314/800, game score: 320.0, reward: -355.0, avg reward: 546.2738853503184, time: 674, total time: 213434\n",
      "episode: 315/800, game score: 410.0, reward: -132.0, avg reward: 545.8412698412699, time: 541, total time: 213976\n",
      "episode: 316/800, game score: 410.0, reward: -231.0, avg reward: 545.4113924050633, time: 640, total time: 214617\n",
      "episode: 317/800, game score: 490.0, reward: -523.0, avg reward: 545.2365930599369, time: 1012, total time: 215630\n",
      "episode: 318/800, game score: 520.0, reward: -82.0, avg reward: 545.1572327044025, time: 601, total time: 216232\n",
      "episode: 319/800, game score: 320.0, reward: -108.0, avg reward: 544.4514106583072, time: 427, total time: 216660\n",
      "episode: 320/800, game score: 390.0, reward: -105.0, avg reward: 543.96875, time: 494, total time: 217155\n",
      "episode: 321/800, game score: 440.0, reward: -146.0, avg reward: 543.6448598130842, time: 585, total time: 217741\n",
      "episode: 322/800, game score: 410.0, reward: -229.0, avg reward: 543.2298136645962, time: 638, total time: 218380\n",
      "episode: 323/800, game score: 690.0, reward: -204.0, avg reward: 543.6842105263158, time: 893, total time: 219274\n",
      "episode: 324/800, game score: 290.0, reward: -328.0, avg reward: 542.9012345679013, time: 617, total time: 219892\n",
      "episode: 325/800, game score: 330.0, reward: -259.0, avg reward: 542.2461538461539, time: 588, total time: 220481\n",
      "episode: 326/800, game score: 670.0, reward: -189.0, avg reward: 542.638036809816, time: 858, total time: 221340\n",
      "episode: 327/800, game score: 1050.0, reward: -72.0, avg reward: 544.1896024464831, time: 1121, total time: 222462\n",
      "episode: 328/800, game score: 530.0, reward: -214.0, avg reward: 544.1463414634146, time: 743, total time: 223206\n",
      "episode: 329/800, game score: 310.0, reward: -170.0, avg reward: 543.434650455927, time: 479, total time: 223686\n",
      "episode: 330/800, game score: 310.0, reward: -242.0, avg reward: 542.7272727272727, time: 551, total time: 224238\n",
      "episode: 331/800, game score: 460.0, reward: -63.0, avg reward: 542.477341389728, time: 522, total time: 224761\n",
      "episode: 332/800, game score: 750.0, reward: -219.0, avg reward: 543.1024096385543, time: 968, total time: 225730\n",
      "episode: 333/800, game score: 790.0, reward: -243.0, avg reward: 543.8438438438438, time: 1032, total time: 226763\n",
      "episode: 334/800, game score: 320.0, reward: -228.0, avg reward: 543.1736526946107, time: 547, total time: 227311\n",
      "episode: 335/800, game score: 300.0, reward: -493.0, avg reward: 542.4477611940298, time: 792, total time: 228104\n",
      "episode: 336/800, game score: 330.0, reward: -222.0, avg reward: 541.8154761904761, time: 551, total time: 228656\n",
      "episode: 337/800, game score: 260.0, reward: -263.0, avg reward: 540.9792284866469, time: 522, total time: 229179\n",
      "episode: 338/800, game score: 340.0, reward: -402.0, avg reward: 540.3846153846154, time: 741, total time: 229921\n",
      "episode: 339/800, game score: 750.0, reward: -27.0, avg reward: 541.0029498525074, time: 776, total time: 230698\n",
      "episode: 340/800, game score: 1160.0, reward: 171.0, avg reward: 542.8235294117648, time: 988, total time: 231687\n",
      "episode: 341/800, game score: 830.0, reward: -145.0, avg reward: 543.6656891495601, time: 974, total time: 232662\n",
      "episode: 342/800, game score: 430.0, reward: -337.0, avg reward: 543.3333333333334, time: 766, total time: 233429\n",
      "episode: 343/800, game score: 230.0, reward: -458.0, avg reward: 542.4198250728863, time: 687, total time: 234117\n",
      "episode: 344/800, game score: 440.0, reward: -402.0, avg reward: 542.1220930232558, time: 841, total time: 234959\n",
      "episode: 345/800, game score: 560.0, reward: -215.0, avg reward: 542.1739130434783, time: 774, total time: 235734\n",
      "episode: 346/800, game score: 450.0, reward: -492.0, avg reward: 541.9075144508671, time: 941, total time: 236676\n",
      "episode: 347/800, game score: 230.0, reward: -233.0, avg reward: 541.0086455331412, time: 462, total time: 237139\n",
      "episode: 348/800, game score: 620.0, reward: -247.0, avg reward: 541.235632183908, time: 866, total time: 238006\n",
      "episode: 349/800, game score: 670.0, reward: -131.0, avg reward: 541.6045845272206, time: 800, total time: 238807\n",
      "episode: 350/800, game score: 300.0, reward: -500.0, avg reward: 540.9142857142857, time: 799, total time: 239607\n",
      "episode: 351/800, game score: 310.0, reward: -169.0, avg reward: 540.2564102564103, time: 478, total time: 240086\n",
      "episode: 352/800, game score: 360.0, reward: -434.0, avg reward: 539.7443181818181, time: 793, total time: 240880\n",
      "episode: 353/800, game score: 210.0, reward: -335.0, avg reward: 538.8101983002833, time: 544, total time: 241425\n",
      "episode: 354/800, game score: 360.0, reward: -159.0, avg reward: 538.3050847457627, time: 518, total time: 241944\n",
      "episode: 355/800, game score: 320.0, reward: -315.0, avg reward: 537.6901408450705, time: 634, total time: 242579\n",
      "episode: 356/800, game score: 360.0, reward: -500.0, avg reward: 537.1910112359551, time: 859, total time: 243439\n",
      "episode: 357/800, game score: 290.0, reward: -376.0, avg reward: 536.498599439776, time: 665, total time: 244105\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-77e520b80d10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-3f0710bb37cf>\u001b[0m in \u001b[0;36mreplay\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[1;31m# 3. Use vectors in the objective computation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon_min\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(episodes):\n",
    "    total_reward = 0\n",
    "    game_score = 0\n",
    "    state = process_frame(env.reset())\n",
    "    images = deque(maxlen=blend)  # Array of images to be blended\n",
    "    images.append(state)\n",
    "    \n",
    "    for skip in range(skip_start): # skip the start of each game\n",
    "        env.step(0)\n",
    "    \n",
    "    for time in range(20000):\n",
    "        env.render()\n",
    "        total_time += 1\n",
    "        \n",
    "        # Every update_rate timesteps we update the target network parameters\n",
    "        if total_time % agent.update_rate == 0:\n",
    "            agent.update_target_model()\n",
    "        \n",
    "        # Return the avg of the last 4 frames\n",
    "        state = blend_images(images, blend)\n",
    "        \n",
    "        # Transition Dynamics\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Return the avg of the last 4 frames\n",
    "        next_state = process_frame(next_state)\n",
    "        images.append(next_state)\n",
    "        next_state = blend_images(images, blend)\n",
    "        \n",
    "        # Store sequence in replay memory\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        \n",
    "        state = next_state\n",
    "        game_score += reward\n",
    "        reward -= 1  # Punish behavior which does not accumulate reward\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            all_rewards += game_score\n",
    "            \n",
    "            print(\"episode: {}/{}, game score: {}, reward: {}, avg reward: {}, time: {}, total time: {}\"\n",
    "                  .format(e+1, episodes, game_score, total_reward, all_rewards/(e+1), time, total_time))\n",
    "            \n",
    "            break\n",
    "            \n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "        agent.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
